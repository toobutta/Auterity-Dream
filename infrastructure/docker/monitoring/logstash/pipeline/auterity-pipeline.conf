# Logstash pipeline configuration for Auterity AI Platform

input {
  # Beats input for Filebeat
  beats {
    port => 5044
    ssl => false
  }

  # HTTP input for direct log ingestion
  http {
    port => 8080
    codec => json
  }

  # TCP input for application logs
  tcp {
    port => 5000
    codec => json_lines
  }
}

filter {
  # Add timestamp if missing
  if ![timestamp] {
    mutate {
      add_field => { "timestamp" => "%{@timestamp}" }
    }
  }

  # Parse timestamp
  date {
    match => ["timestamp", "ISO8601", "yyyy-MM-dd'T'HH:mm:ss.SSS'Z'", "yyyy-MM-dd HH:mm:ss"]
    target => "@timestamp"
    timezone => "UTC"
  }

  # Add service information based on source
  if [kubernetes][container][name] =~ /^auterity-/ {
    mutate {
      add_field => { "service" => "%{[kubernetes][container][name]}" }
      add_field => { "environment" => "production" }
      add_field => { "platform" => "auterity-ai" }
    }
  }

  # Parse log levels
  grok {
    match => { "message" => "%{LOGLEVEL:level} %{GREEDYDATA:log_message}" }
    patterns_dir => ["/usr/share/logstash/patterns"]
    overwrite => ["message"]
  }

  # Extract JSON fields from message if it's JSON
  if [message] =~ /^\{/ {
    json {
      source => "message"
      target => "parsed_json"
    }

    # Move parsed fields to root level
    if [parsed_json] {
      ruby {
        code => "
          event.get('parsed_json').each do |key, value|
            event.set(key, value) unless event.get(key)
          end
          event.remove('parsed_json')
        "
      }
    }
  }

  # Add geoip information for IP addresses
  if [client_ip] {
    geoip {
      source => "client_ip"
      target => "geoip"
    }
  }

  # Extract user context
  if [user_id] {
    mutate {
      add_field => { "user_context" => "%{user_id}" }
    }
  }

  # Extract request context
  if [request_id] {
    mutate {
      add_field => { "request_context" => "%{request_id}" }
    }
  }

  # Categorize logs by type
  if [level] == "error" or [level] == "ERROR" {
    mutate {
      add_tag => ["error"]
      add_field => { "log_category" => "error" }
    }
  } else if [level] == "warn" or [level] == "WARN" {
    mutate {
      add_tag => ["warning"]
      add_field => { "log_category" => "warning" }
    }
  } else if [level] == "info" or [level] == "INFO" {
    mutate {
      add_tag => ["info"]
      add_field => { "log_category" => "info" }
    }
  } else {
    mutate {
      add_field => { "log_category" => "unknown" }
    }
  }

  # Extract error patterns
  if [log_category] == "error" {
    grok {
      match => { "message" => "%{WORD:service_name}: %{GREEDYDATA:error_message}" }
      add_field => { "error_service" => "%{service_name}" }
      add_field => { "error_details" => "%{error_message}" }
    }

    # Extract stack traces
    if [message] =~ /at\s+\w+\.\w+/ {
      mutate {
        add_field => { "has_stack_trace" => true }
      }
    }
  }

  # Performance monitoring
  if [duration] {
    mutate {
      convert => { "duration" => "float" }
    }

    # Categorize response times
    if [duration] > 5000 {
      mutate {
        add_tag => ["slow_response"]
        add_field => { "performance_category" => "slow" }
      }
    } else if [duration] > 1000 {
      mutate {
        add_field => { "performance_category" => "medium" }
      }
    } else {
      mutate {
        add_field => { "performance_category" => "fast" }
      }
    }
  }

  # AI-specific processing
  if [service] =~ /vllm|langgraph|crewai/ {
    mutate {
      add_field => { "component_type" => "ai_service" }
    }

    # Extract AI model information
    if [model] {
      mutate {
        add_field => { "ai_model" => "%{model}" }
      }
    }

    # Extract token usage
    if [tokens] {
      mutate {
        convert => { "tokens" => "integer" }
        add_field => { "token_count" => "%{tokens}" }
      }
    }

    # Extract cost information
    if [cost] {
      mutate {
        convert => { "cost" => "float" }
        add_field => { "processing_cost" => "%{cost}" }
      }
    }
  }

  # Workflow processing
  if [workflow_id] {
    mutate {
      add_field => { "component_type" => "workflow" }
      add_field => { "workflow_context" => "%{workflow_id}" }
    }

    # Extract workflow status
    if [status] {
      mutate {
        add_field => { "workflow_status" => "%{status}" }
      }
    }
  }

  # Agent processing
  if [agent_id] {
    mutate {
      add_field => { "component_type" => "agent" }
      add_field => { "agent_context" => "%{agent_id}" }
    }
  }

  # Remove sensitive information
  mutate {
    remove_field => ["password", "secret", "token", "key", "auth"]
  }

  # Add processing metadata
  mutate {
    add_field => {
      "processed_at" => "%{@timestamp}"
      "processing_node" => "%{host}"
      "logstash_version" => "8.11.0"
    }
  }
}

output {
  # Main Elasticsearch output
  elasticsearch {
    hosts => ["elasticsearch:9200"]
    index => "auterity-ai-%{+YYYY.MM.dd}"
    document_id => "%{[@metadata][document_id]}"
    user => "${ELASTICSEARCH_USERNAME:elastic}"
    password => "${ELASTIC_PASSWORD}"
    ssl => false
    ilm_enabled => true
    ilm_rollover_alias => "auterity-ai"
    ilm_policy => "auterity-ai-policy"
  }

  # Error logs to separate index
  if [log_category] == "error" {
    elasticsearch {
      hosts => ["elasticsearch:9200"]
      index => "auterity-ai-errors-%{+YYYY.MM.dd}"
      user => "${ELASTICSEARCH_USERNAME:elastic}"
      password => "${ELASTIC_PASSWORD}"
      ssl => false
    }
  }

  # Performance logs to separate index
  if [performance_category] {
    elasticsearch {
      hosts => ["elasticsearch:9200"]
      index => "auterity-ai-performance-%{+YYYY.MM.dd}"
      user => "${ELASTICSEARCH_USERNAME:elastic}"
      password => "${ELASTIC_PASSWORD}"
      ssl => false
    }
  }

  # AI-specific logs
  if [component_type] == "ai_service" {
    elasticsearch {
      hosts => ["elasticsearch:9200"]
      index => "auterity-ai-models-%{+YYYY.MM.dd}"
      user => "${ELASTICSEARCH_USERNAME:elastic}"
      password => "${ELASTIC_PASSWORD}"
      ssl => false
    }
  }

  # Workflow logs
  if [component_type] == "workflow" {
    elasticsearch {
      hosts => ["elasticsearch:9200"]
      index => "auterity-ai-workflows-%{+YYYY.MM.dd}"
      user => "${ELASTICSEARCH_USERNAME:elastic}"
      password => "${ELASTIC_PASSWORD}"
      ssl => false
    }
  }

  # Debug output (can be disabled in production)
  if [level] == "debug" {
    stdout {
      codec => rubydebug
    }
  }

  # Metrics output to Prometheus (optional)
  if [metrics] {
    http {
      url => "http://prometheus:9090/api/v1/write"
      http_method => "post"
      format => "json"
      message_format => "%{metrics}"
    }
  }
}

