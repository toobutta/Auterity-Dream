version: '3.8'

services:
  vllm-server:
    build:
      context: ../../systems/vllm
      dockerfile: Dockerfile
    container_name: auterity-vllm
    ports:
      - "${VLLM_PORT:-8001}:8001"
    environment:
      # Model Configuration
      - VLLM_MODEL_NAME=${VLLM_MODEL_NAME:-meta-llama/Llama-2-7b-chat-hf}
      - VLLM_TENSOR_PARALLEL_SIZE=${VLLM_TENSOR_PARALLEL_SIZE:-1}
      - VLLM_GPU_MEMORY_UTILIZATION=${VLLM_GPU_MEMORY_UTILIZATION:-0.9}
      - VLLM_MAX_MODEL_LEN=${VLLM_MAX_MODEL_LEN:-4096}
      - VLLM_MAX_BATCH_SIZE=${VLLM_MAX_BATCH_SIZE:-32}
      - VLLM_DTYPE=${VLLM_DTYPE:-auto}
      - VLLM_QUANTIZATION=${VLLM_QUANTIZATION:-}

      # Redis Configuration
      - REDIS_HOST=${REDIS_HOST:-redis}
      - REDIS_PORT=${REDIS_PORT:-6379}
      - REDIS_DB=${REDIS_DB:-3}

      # Service Configuration
      - SERVICE_NAME=vllm-server
      - LOG_LEVEL=${LOG_LEVEL:-INFO}

    volumes:
      # Mount model cache for persistence
      - vllm_model_cache:/root/.cache/huggingface
      # Mount logs for monitoring
      - vllm_logs:/app/logs

    # GPU Configuration
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: ${VLLM_GPU_COUNT:-1}
              capabilities: [gpu]
        limits:
          memory: ${VLLM_MEMORY_LIMIT:-16G}
      restart_policy:
        condition: on-failure
        delay: 5s
        max_attempts: 3
        window: 120s

    # Health checks
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8001/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s

    # Networking
    networks:
      - auterity-network

    # Logging
    logging:
      driver: json-file
      options:
        max-size: "100m"
        max-file: "5"

    # Dependencies
    depends_on:
      redis:
        condition: service_healthy

  # Optional: Model Warm-up Service
  vllm-warmup:
    build:
      context: ../../systems/vllm
      dockerfile: Dockerfile
    container_name: auterity-vllm-warmup
    environment:
      - VLLM_MODEL_NAME=${VLLM_MODEL_NAME:-meta-llama/Llama-2-7b-chat-hf}
      - WARMUP_MODE=true
    volumes:
      - vllm_model_cache:/root/.cache/huggingface
    command: ["python", "src/warmup.py"]
    profiles:
      - warmup
    networks:
      - auterity-network
    depends_on:
      - vllm-server

  # Optional: Monitoring Sidecar
  vllm-monitor:
    image: prom/prometheus:latest
    container_name: auterity-vllm-monitor
    ports:
      - "9090:9090"
    volumes:
      - ./monitoring/prometheus.yml:/etc/prometheus/prometheus.yml
      - vllm_metrics:/prometheus
    command:
      - '--config.file=/etc/prometheus/prometheus.yml'
      - '--storage.tsdb.path=/prometheus'
      - '--web.console.libraries=/etc/prometheus/console_libraries'
      - '--web.console.templates=/etc/prometheus/consoles'
      - '--storage.tsdb.retention.time=200h'
      - '--web.enable-lifecycle'
    profiles:
      - monitoring
    networks:
      - auterity-network

volumes:
  vllm_model_cache:
    driver: local
    driver_opts:
      type: none
      o: bind
      device: ${VLLM_CACHE_DIR:-./data/vllm/cache}

  vllm_logs:
    driver: local
    driver_opts:
      type: none
      o: bind
      device: ${VLLM_LOG_DIR:-./data/vllm/logs}

  vllm_metrics:
    driver: local

networks:
  auterity-network:
    external: true
    name: auterity-network

