# Model configurations for LiteLLM service
models:
  # OpenAI models
  - name: gpt-3.5-turbo
    provider: openai
    max_tokens: 4096
    cost_per_token: 0.000002
    capabilities:
      - text
      - chat
      - function_calling
    is_available: true

  - name: gpt-4
    provider: openai
    max_tokens: 8192
    cost_per_token: 0.00003
    capabilities:
      - text
      - chat
      - function_calling
      - reasoning
    is_available: true

  - name: gpt-4-turbo-preview
    provider: openai
    max_tokens: 4096
    cost_per_token: 0.00001
    capabilities:
      - text
      - chat
      - function_calling
      - reasoning
    is_available: true

  # Anthropic models
  - name: claude-2
    provider: anthropic
    max_tokens: 100000
    cost_per_token: 0.00001
    capabilities:
      - text
      - chat
      - reasoning
      - long_context
    is_available: true

  - name: claude-instant-1
    provider: anthropic
    max_tokens: 100000
    cost_per_token: 0.000002
    capabilities:
      - text
      - chat
    is_available: true

  # Ollama models (local deployment)
  - name: llama2
    provider: ollama
    endpoint: http://localhost:11434
    max_tokens: 4096
    capabilities:
      - text
      - chat
    is_available: false  # Disabled by default, enable when Ollama is running
