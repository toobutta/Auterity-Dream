name: Optimized CI/CD Pipeline

on:
  push:
    branches: [main, develop]
    paths-ignore:
      - '**/*.md'
      - 'docs/**'
      - '.gitignore'
      - 'LICENSE'
  pull_request:
    branches: [main, develop]
    paths-ignore:
      - '**/*.md'
      - 'docs/**'
      - '.gitignore'
      - 'LICENSE'

concurrency:
  group: ci-${{ github.ref }}
  cancel-in-progress: true

env:
  NODE_VERSION: '18'
  PYTHON_VERSION: '3.12'
  CACHE_VERSION: 'v1'
  QUALITY_GATE_BLOCKING: true
  SECURITY_THRESHOLD: 'HIGH'
  COVERAGE_THRESHOLD: 80
  PERFORMANCE_THRESHOLD: 2000

jobs:
  # Intelligent change detection and setup
  setup-matrix:
    runs-on: ubuntu-latest
    outputs:
      backend-changed: ${{ steps.changes.outputs.backend }}
      frontend-changed: ${{ steps.changes.outputs.frontend }}
      relaycore-changed: ${{ steps.changes.outputs.relaycore }}
      neuroweaver-changed: ${{ steps.changes.outputs.neuroweaver }}
      docker-changed: ${{ steps.changes.outputs.docker }}
      config-changed: ${{ steps.changes.outputs.config }}
      security-scan-needed: ${{ steps.security-check.outputs.needed }}
      performance-test-needed: ${{ steps.performance-check.outputs.needed }}
      backend-cache-key: ${{ steps.cache-keys.outputs.backend }}
      frontend-cache-key: ${{ steps.cache-keys.outputs.frontend }}
      should-deploy: ${{ steps.deploy-check.outputs.should-deploy }}
    steps:
      - uses: actions/checkout@v4
        with:
          fetch-depth: 0

      - name: Detect changes
        uses: dorny/paths-filter@v2
        id: changes
        with:
          filters: |
            backend:
              - 'backend/**'
              - 'scripts/**'
              - 'requirements*.txt'
            frontend:
              - 'frontend/**'
              - 'shared/**'
              - 'package*.json'
            relaycore:
              - 'systems/relaycore/**'
            neuroweaver:
              - 'systems/neuroweaver/**'
            docker:
              - 'Dockerfile*'
              - 'docker-compose*.yml'
              - '.dockerignore'
            config:
              - 'config/**'
              - '.github/workflows/**'
              - 'nginx/**'
              - 'kong/**'

      - name: Generate cache keys
        id: cache-keys
        run: |
          backend_key="${{ env.CACHE_VERSION }}-backend-${{ hashFiles('backend/requirements*.txt', 'backend/pyproject.toml') }}"
          frontend_key="${{ env.CACHE_VERSION }}-frontend-${{ hashFiles('frontend/package*.json', 'package*.json') }}"

          echo "backend=$backend_key" >> $GITHUB_OUTPUT
          echo "frontend=$frontend_key" >> $GITHUB_OUTPUT

      - name: Check security scan requirements
        id: security-check
        run: |
          # Always run security scan on main/develop branches
          if [[ "${{ github.ref }}" == "refs/heads/main" || "${{ github.ref }}" == "refs/heads/develop" ]]; then
            echo "needed=true" >> $GITHUB_OUTPUT
          # Run on PR if security-relevant files changed
          elif [[ "${{ steps.changes.outputs.backend }}" == "true" || "${{ steps.changes.outputs.frontend }}" == "true" || "${{ steps.changes.outputs.config }}" == "true" ]]; then
            echo "needed=true" >> $GITHUB_OUTPUT
          else
            echo "needed=false" >> $GITHUB_OUTPUT
          fi

      - name: Check performance test requirements
        id: performance-check
        run: |
          # Run performance tests on main/develop or if relevant files changed
          if [[ "${{ github.ref }}" == "refs/heads/main" || "${{ github.ref }}" == "refs/heads/develop" || "${{ steps.changes.outputs.frontend }}" == "true" || "${{ steps.changes.outputs.backend }}" == "true" ]]; then
            echo "needed=true" >> $GITHUB_OUTPUT
          else
            echo "needed=false" >> $GITHUB_OUTPUT
          fi

      - name: Check deployment requirements
        id: deploy-check
        run: |
          # Deploy only on main branch or if docker/config files changed
          if [[ "${{ github.ref }}" == "refs/heads/main" && "${{ github.event_name }}" == "push" ]]; then
            echo "should-deploy=true" >> $GITHUB_OUTPUT
          else
            echo "should-deploy=false" >> $GITHUB_OUTPUT
          fi

  # Optimized linting and formatting with caching
  code-quality:
    runs-on: ubuntu-latest
    needs: setup-matrix
    if: needs.setup-matrix.outputs.frontend-changed == 'true' || needs.setup-matrix.outputs.backend-changed == 'true'
    steps:
      - uses: actions/checkout@v4

      - name: Setup Node.js
        if: needs.setup-matrix.outputs.frontend-changed == 'true'
        uses: actions/setup-node@v4
        with:
          node-version: ${{ env.NODE_VERSION }}
          cache: 'npm'
          cache-dependency-path: |
            package-lock.json
            frontend/package-lock.json

      - name: Setup Python
        if: needs.setup-matrix.outputs.backend-changed == 'true'
        uses: actions/setup-python@v4
        with:
          python-version: ${{ env.PYTHON_VERSION }}
          cache: 'pip'
          cache-dependency-path: backend/requirements*.txt

      - name: Cache Python packages
        if: needs.setup-matrix.outputs.backend-changed == 'true'
        uses: actions/cache@v3
        with:
          path: ~/.cache/pip
          key: ${{ needs.setup-matrix.outputs.backend-cache-key }}
          restore-keys: |
            ${{ env.CACHE_VERSION }}-backend-

      - name: Cache Node modules
        if: needs.setup-matrix.outputs.frontend-changed == 'true'
        uses: actions/cache@v3
        with:
          path: |
            node_modules
            frontend/node_modules
            ~/.npm
          key: ${{ needs.setup-matrix.outputs.frontend-cache-key }}
          restore-keys: |
            ${{ env.CACHE_VERSION }}-frontend-

      - name: Install dependencies
        run: |
          if [[ "${{ needs.setup-matrix.outputs.frontend-changed }}" == "true" ]]; then
            npm ci
            cd frontend && npm ci && cd ..
          fi

          if [[ "${{ needs.setup-matrix.outputs.backend-changed }}" == "true" ]]; then
            cd backend
            pip install -r requirements.txt -r requirements-dev.txt
            cd ..
          fi

      - name: Run frontend quality checks
        if: needs.setup-matrix.outputs.frontend-changed == 'true'
        run: |
          cd frontend

          # Type checking
          echo "üîç Running TypeScript type checking..."
          npx tsc --noEmit --incremental

          # Linting with caching
          echo "üîß Running ESLint..."
          npx eslint src --ext .ts,.tsx,.js,.jsx --cache --cache-location .eslintcache --max-warnings 0

          # Code formatting check
          echo "üìù Checking code formatting..."
          npx prettier --check "src/**/*.{ts,tsx,js,jsx,json,css,scss}"

          # Import sorting
          echo "üì¶ Checking import sorting..."
          npx organize-imports-cli --check src/**/*.{ts,tsx}

          echo "‚úÖ Frontend quality checks passed"

      - name: Run backend quality checks
        if: needs.setup-matrix.outputs.backend-changed == 'true'
        run: |
          cd backend

          # Code formatting check
          echo "üìù Checking code formatting..."
          python -m black --check --diff .

          # Import sorting
          echo "üì¶ Checking import sorting..."
          python -m isort --check-only --diff .

          # Linting
          echo "üîß Running flake8..."
          python -m flake8 app tests

          # Type checking
          echo "üîç Running mypy type checking..."
          python -m mypy app --ignore-missing-imports --strict

          # Additional linting with pylint
          echo "üîß Running pylint..."
          python -m pylint app --disable=C0114,C0115,C0116 --exit-zero

          echo "‚úÖ Backend quality checks passed"

      - name: Upload quality reports
        if: always()
        uses: actions/upload-artifact@v3
        with:
          name: quality-reports
          path: |
            frontend/.eslintcache
            backend/.mypy_cache
          retention-days: 1

  # Parallel testing with intelligent caching
  test-backend:
    runs-on: ubuntu-latest
    needs: [setup-matrix, code-quality]
    if: needs.setup-matrix.outputs.backend-changed == 'true'
    services:
      postgres:
        image: postgres:15-alpine
        env:
          POSTGRES_PASSWORD: postgres
          POSTGRES_DB: auterity_test
        options: >-
          --health-cmd pg_isready
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5
        ports:
          - 5432:5432
      redis:
        image: redis:7-alpine
        options: >-
          --health-cmd "redis-cli ping"
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5
        ports:
          - 6379:6379
    steps:
      - uses: actions/checkout@v4

      - name: Setup Python
        uses: actions/setup-python@v4
        with:
          python-version: ${{ env.PYTHON_VERSION }}
          cache: 'pip'
          cache-dependency-path: backend/requirements*.txt

      - name: Cache Python packages
        uses: actions/cache@v3
        with:
          path: ~/.cache/pip
          key: ${{ needs.setup-matrix.outputs.backend-cache-key }}
          restore-keys: |
            ${{ env.CACHE_VERSION }}-backend-

      - name: Install dependencies
        run: |
          cd backend
          pip install -r requirements.txt -r requirements-dev.txt

      - name: Set up test database
        run: |
          cd backend
          export DATABASE_URL="postgresql://postgres:postgres@localhost:5432/auterity_test"
          export REDIS_URL="redis://localhost:6379"
          alembic upgrade head

      - name: Run tests with coverage
        run: |
          cd backend
          export DATABASE_URL="postgresql://postgres:postgres@localhost:5432/auterity_test"
          export REDIS_URL="redis://localhost:6379"
          export TESTING=true

          python -m pytest \
            tests/ \
            -v \
            --cov=app \
            --cov-report=xml \
            --cov-report=html \
            --cov-report=term \
            --cov-fail-under=${{ env.COVERAGE_THRESHOLD }} \
            --junitxml=test-results.xml \
            --tb=short \
            --maxfail=10 \
            --durations=10

      - name: Upload test results
        if: always()
        uses: actions/upload-artifact@v3
        with:
          name: backend-test-results
          path: |
            backend/test-results.xml
            backend/htmlcov/
            backend/coverage.xml
          retention-days: 7

      - name: Upload coverage to Codecov
        uses: codecov/codecov-action@v3
        with:
          file: backend/coverage.xml
          flags: backend
          name: backend-coverage

  test-frontend:
    runs-on: ubuntu-latest
    needs: [setup-matrix, code-quality]
    if: needs.setup-matrix.outputs.frontend-changed == 'true'
    steps:
      - uses: actions/checkout@v4

      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: ${{ env.NODE_VERSION }}
          cache: 'npm'
          cache-dependency-path: |
            package-lock.json
            frontend/package-lock.json

      - name: Cache Node modules
        uses: actions/cache@v3
        with:
          path: |
            node_modules
            frontend/node_modules
            ~/.npm
          key: ${{ needs.setup-matrix.outputs.frontend-cache-key }}
          restore-keys: |
            ${{ env.CACHE_VERSION }}-frontend-

      - name: Install dependencies
        run: |
          npm ci
          cd frontend && npm ci

      - name: Run unit tests
        run: |
          cd frontend
          npm test -- \
            --coverage \
            --coverageThreshold='{"global":{"branches":${{ env.COVERAGE_THRESHOLD }},"functions":${{ env.COVERAGE_THRESHOLD }},"lines":${{ env.COVERAGE_THRESHOLD }},"statements":${{ env.COVERAGE_THRESHOLD }}}}' \
            --watchAll=false \
            --maxWorkers=4 \
            --verbose

      - name: Run component tests
        run: |
          cd frontend
          npm run test:components || echo "Component tests not configured"

      - name: Build application
        run: |
          cd frontend
          npm run build

      - name: Upload test results
        if: always()
        uses: actions/upload-artifact@v3
        with:
          name: frontend-test-results
          path: |
            frontend/coverage/
            frontend/build/
          retention-days: 7

      - name: Upload coverage to Codecov
        uses: codecov/codecov-action@v3
        with:
          file: frontend/coverage/lcov.info
          flags: frontend
          name: frontend-coverage

  # Comprehensive security scanning
  security-scan:
    runs-on: ubuntu-latest
    needs: setup-matrix
    if: needs.setup-matrix.outputs.security-scan-needed == 'true'
    permissions:
      security-events: write
      actions: read
      contents: read
    steps:
      - uses: actions/checkout@v4
        with:
          fetch-depth: 0

      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: ${{ env.NODE_VERSION }}

      - name: Setup Python
        uses: actions/setup-python@v4
        with:
          python-version: ${{ env.PYTHON_VERSION }}

      - name: Install security tools
        run: |
          npm install -g npm-audit-resolver snyk
          pip install bandit safety pip-audit semgrep

      - name: Run Trivy filesystem scan
        uses: aquasecurity/trivy-action@master
        with:
          scan-type: 'fs'
          scan-ref: '.'
          format: 'sarif'
          output: 'trivy-results.sarif'
          severity: 'CRITICAL,HIGH,MEDIUM'

      - name: Upload Trivy scan results
        uses: github/codeql-action/upload-sarif@v2
        if: always()
        with:
          sarif_file: 'trivy-results.sarif'

      - name: Run CodeQL Analysis
        uses: github/codeql-action/init@v2
        with:
          languages: javascript,python

      - name: Perform CodeQL Analysis
        uses: github/codeql-action/analyze@v2

      - name: Run Security Scans
        run: |
          python .github/scripts/security-scan.py

      - name: Generate security report
        run: |
          echo "# üîí Security Scan Report" > security-report.md
          echo "" >> security-report.md
          echo "**Timestamp:** $(date)" >> security-report.md
          echo "**Commit:** ${{ github.sha }}" >> security-report.md
          echo "**Branch:** ${{ github.ref_name }}" >> security-report.md
          echo "" >> security-report.md

          echo "## Scan Results" >> security-report.md
          echo "" >> security-report.md

          # Process frontend results
          if [[ -f "frontend/npm-audit.json" ]]; then
            npm_vulns=$(cat frontend/npm-audit.json | jq '.metadata.vulnerabilities.total // 0')
            echo "- **NPM Vulnerabilities:** $npm_vulns" >> security-report.md
          fi

          # Process backend results
          if [[ -f "backend/bandit-results.json" ]]; then
            bandit_issues=$(cat backend/bandit-results.json | jq '.results | length')
            echo "- **Bandit Security Issues:** $bandit_issues" >> security-report.md
          fi

          if [[ -f "backend/safety-results.json" ]]; then
            safety_vulns=$(cat backend/safety-results.json | jq '. | length')
            echo "- **Safety Vulnerabilities:** $safety_vulns" >> security-report.md
          fi

      - name: Upload security artifacts
        if: always()
        uses: actions/upload-artifact@v3
        with:
          name: security-scan-results
          path: |
            trivy-results.sarif
            frontend/npm-audit.json
            frontend/snyk-results.json
            backend/bandit-results.json
            backend/safety-results.json
            backend/pip-audit.json
            backend/semgrep-results.json
            security-report.md
          retention-days: 30

  # Optimized performance testing
  performance-test:
    runs-on: ubuntu-latest
    needs: [setup-matrix, test-backend, test-frontend]
    if: needs.setup-matrix.outputs.performance-test-needed == 'true'
    steps:
      - uses: actions/checkout@v4

      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: ${{ env.NODE_VERSION }}

      - name: Setup Python
        uses: actions/setup-python@v4
        with:
          python-version: ${{ env.PYTHON_VERSION }}

      - name: Install performance tools
        run: |
          npm install -g lighthouse bundlesize webpack-bundle-analyzer
          pip install locust

      - name: Run Integration Tests
        run: |
          bash .github/scripts/integration-tests.sh

      - name: Download build artifacts
        uses: actions/download-artifact@v3
        with:
          name: frontend-test-results
          path: frontend/

      - name: Bundle size analysis
        run: |
          cd frontend

          # Analyze bundle size
          if [[ -d "build/static/js" ]]; then
            total_size=$(du -sb build/static/js/*.js | awk '{sum += $1} END {print sum}')
            echo "Total JS bundle size: $total_size bytes"

            # Check against limit (5MB)
            max_size=$((5 * 1024 * 1024))
            if [[ $total_size -gt $max_size ]]; then
              echo "‚ùå Bundle size exceeds limit"
              exit 1
            fi

            # Generate bundle analysis
            npx webpack-bundle-analyzer build/static/js/*.js --mode static --report bundle-analysis.html --no-open || true
          fi

      - name: Start application for testing
        run: |
          # Start services with Docker Compose
          docker-compose -f docker-compose.yml up -d --build

          # Wait for services to be ready
          timeout 120 bash -c 'until curl -f http://localhost:3000/health; do sleep 5; done'
          timeout 120 bash -c 'until curl -f http://localhost:8000/health; do sleep 5; done'

      - name: Run Lighthouse performance audit
        run: |
          # Run Lighthouse on key pages
          lighthouse http://localhost:3000 \
            --output=json \
            --output-path=lighthouse-home.json \
            --chrome-flags="--headless --no-sandbox" \
            --quiet

          lighthouse http://localhost:3000/dashboard \
            --output=json \
            --output-path=lighthouse-dashboard.json \
            --chrome-flags="--headless --no-sandbox" \
            --quiet || true

      - name: API load testing
        run: |
          cat > locustfile.py << 'EOF'
          from locust import HttpUser, task, between
          import json

          class APIUser(HttpUser):
              wait_time = between(1, 3)
              host = "http://localhost:8000"

              def on_start(self):
                  self.client.verify = False

              @task(3)
              def get_workflows(self):
                  response = self.client.get("/api/workflows/")
                  if response.status_code != 200:
                      print(f"Workflow endpoint failed: {response.status_code}")

              @task(2)
              def health_check(self):
                  self.client.get("/health")

              @task(1)
              def get_status(self):
                  self.client.get("/api/status")
          EOF

          # Run load test
          locust -f locustfile.py \
            --host=http://localhost:8000 \
            --users=20 \
            --spawn-rate=4 \
            --run-time=60s \
            --csv=locust_results \
            --headless

      - name: Analyze performance results
        run: |
          # Analyze Lighthouse results
          if [[ -f "lighthouse-home.json" ]]; then
            perf_score=$(cat lighthouse-home.json | jq '.categories.performance.score * 100')
            echo "Lighthouse Performance Score: $perf_score"

            if (( $(echo "$perf_score < 70" | bc -l) )); then
              echo "‚ùå Performance score below threshold"
            fi
          fi

          # Analyze load test results
          if [[ -f "locust_results_stats.csv" ]]; then
            avg_response_time=$(tail -1 locust_results_stats.csv | cut -d',' -f6)
            echo "Average API response time: ${avg_response_time}ms"

            if (( $(echo "$avg_response_time > ${{ env.PERFORMANCE_THRESHOLD }}" | bc -l) )); then
              echo "‚ùå API response time above threshold"
            fi
          fi

      - name: Cleanup test environment
        if: always()
        run: docker-compose down

      - name: Upload performance results
        if: always()
        uses: actions/upload-artifact@v3
        with:
          name: performance-results
          path: |
            lighthouse-*.json
            locust_results_*.csv
            frontend/bundle-analysis.html
          retention-days: 7

  # Quality gate summary
  quality-gate:
    runs-on: ubuntu-latest
    needs: [setup-matrix, code-quality, test-backend, test-frontend, security-scan, performance-test]
    if: always()
    outputs:
      status: ${{ steps.gate-check.outputs.status }}
    steps:
      - name: Evaluate quality gates
        id: gate-check
        run: |
          echo "üéØ Evaluating quality gates..."

          # Check all job results
          code_quality="${{ needs.code-quality.result }}"
          test_backend="${{ needs.test-backend.result }}"
          test_frontend="${{ needs.test-frontend.result }}"
          security_scan="${{ needs.security-scan.result }}"
          performance_test="${{ needs.performance-test.result }}"

          # Track failures
          failures=()

          if [[ "$code_quality" == "failure" ]]; then
            failures+=("Code Quality")
          fi

          if [[ "$test_backend" == "failure" ]]; then
            failures+=("Backend Tests")
          fi

          if [[ "$test_frontend" == "failure" ]]; then
            failures+=("Frontend Tests")
          fi

          if [[ "$security_scan" == "failure" ]]; then
            failures+=("Security Scan")
          fi

          if [[ "$performance_test" == "failure" ]]; then
            failures+=("Performance Test")
          fi

          # Generate report
          echo "## üîç Quality Gate Report" > quality-gate-report.md
          echo "" >> quality-gate-report.md
          echo "**Timestamp:** $(date)" >> quality-gate-report.md
          echo "**Commit:** ${{ github.sha }}" >> quality-gate-report.md
          echo "**Branch:** ${{ github.ref_name }}" >> quality-gate-report.md
          echo "" >> quality-gate-report.md

          if [[ ${#failures[@]} -eq 0 ]]; then
            echo "‚úÖ **Overall Status: PASSED**" >> quality-gate-report.md
            echo "status=passed" >> $GITHUB_OUTPUT
          else
            echo "‚ùå **Overall Status: FAILED**" >> quality-gate-report.md
            echo "" >> quality-gate-report.md
            echo "**Failed Gates:**" >> quality-gate-report.md
            for failure in "${failures[@]}"; do
              echo "- $failure" >> quality-gate-report.md
            done
            echo "status=failed" >> $GITHUB_OUTPUT
          fi

          echo "" >> quality-gate-report.md
          echo "### Gate Details" >> quality-gate-report.md
          echo "- **Code Quality:** $([[ "$code_quality" == "success" ]] && echo "‚úÖ PASSED" || echo "‚ùå FAILED")" >> quality-gate-report.md
          echo "- **Backend Tests:** $([[ "$test_backend" == "success" ]] && echo "‚úÖ PASSED" || echo "‚ùå FAILED")" >> quality-gate-report.md
          echo "- **Frontend Tests:** $([[ "$test_frontend" == "success" ]] && echo "‚úÖ PASSED" || echo "‚ùå FAILED")" >> quality-gate-report.md
          echo "- **Security Scan:** $([[ "$security_scan" == "success" ]] && echo "‚úÖ PASSED" || echo "‚ùå FAILED")" >> quality-gate-report.md
          echo "- **Performance Test:** $([[ "$performance_test" == "success" ]] && echo "‚úÖ PASSED" || echo "‚ùå FAILED")" >> quality-gate-report.md

      - name: Upload quality gate report
        if: always()
        uses: actions/upload-artifact@v3
        with:
          name: quality-gate-report
          path: quality-gate-report.md
          retention-days: 30

      - name: Enforce quality gate
        run: |
          if [[ "${{ steps.gate-check.outputs.status }}" == "failed" ]]; then
            echo "üî¥ Quality gates failed!"
            echo ""
            echo "Please check the detailed reports and fix the issues before proceeding."
            echo "You can find detailed reports in the workflow artifacts."

            if [[ "${{ env.QUALITY_GATE_BLOCKING }}" == "true" ]]; then
              echo ""
              echo "üí• Pipeline blocked due to quality gate failures."
              exit 1
            else
              echo ""
              echo "‚ö†Ô∏è Quality gates failed but blocking is disabled."
            fi
          else
            echo "üü¢ All quality gates passed! ‚ú®"
          fi

  # Deployment job (only on main branch)
  deploy:
    runs-on: ubuntu-latest
    needs: [setup-matrix, quality-gate]
    if: needs.setup-matrix.outputs.should-deploy == 'true' && needs.quality-gate.outputs.status == 'passed'
    environment: production
    steps:
      - uses: actions/checkout@v4

      - name: Deploy to production
        run: |
          echo "üöÄ Deploying to production..."
          echo "This would trigger your deployment pipeline"

          # Example deployment commands
          # docker build -t auterity-app:${{ github.sha }} .
          # docker push registry/auterity-app:${{ github.sha }}
          # kubectl set image deployment/auterity-app app=registry/auterity-app:${{ github.sha }}

      - name: Create deployment notification
        run: |
          echo "‚úÖ Deployment completed successfully!"
          echo "Version: ${{ github.sha }}"
          echo "Environment: production"
