# Backend Architecture Specification

## Overview

The Auterity backend is a modern Python-based API server built with FastAPI, designed for high performance, scalability, and maintainability. The architecture follows microservices principles with clear separation of concerns and domain-driven design patterns.

## Architecture Diagram

```
┌─────────────────────────────────────────────────────────────────┐
│                      Client Layer                               │
│  ┌─────────────┐  ┌─────────────┐  ┌─────────────┐            │
│  │   Web App   │  │ Mobile App  │  │ Third Party │            │
│  │   React     │  │   Native    │  │    APIs     │            │
│  └─────────────┘  └─────────────┘  └─────────────┘            │
├─────────────────────────────────────────────────────────────────┤
│                       API Gateway                               │
│  ┌─────────────┐  ┌─────────────┐  ┌─────────────┐            │
│  │ Rate Limit  │  │    CORS     │  │    Auth     │            │
│  │ Middleware  │  │ Middleware  │  │ Middleware  │            │
│  └─────────────┘  └─────────────┘  └─────────────┘            │
├─────────────────────────────────────────────────────────────────┤
│                    FastAPI Application                          │
│  ┌─────────────┐  ┌─────────────┐  ┌─────────────┐            │
│  │   Auth API  │  │Workflow API │  │Template API │            │
│  │             │  │             │  │             │            │
│  │ /auth/*     │  │/workflows/* │  │/templates/* │            │
│  └─────────────┘  └─────────────┘  └─────────────┘            │
│                                                                 │
│  ┌─────────────┐  ┌─────────────┐  ┌─────────────┐            │
│  │Execution API│  │Monitoring   │  │ WebSocket   │            │
│  │             │  │    API      │  │   Server    │            │
│  │/executions/*│  │ /metrics/*  │  │   /ws/*     │            │
│  └─────────────┘  └─────────────┘  └─────────────┘            │
├─────────────────────────────────────────────────────────────────┤
│                     Service Layer                               │
│  ┌─────────────┐  ┌─────────────┐  ┌─────────────┐            │
│  │  Workflow   │  │  Template   │  │ AI Service  │            │
│  │   Engine    │  │   Engine    │  │             │            │
│  │             │  │             │  │ OpenAI GPT  │            │
│  └─────────────┘  └─────────────┘  └─────────────┘            │
│                                                                 │
│  ┌─────────────┐  ┌─────────────┐  ┌─────────────┐            │
│  │    Auth     │  │   Email     │  │   File      │            │
│  │  Service    │  │  Service    │  │  Service    │            │
│  │             │  │             │  │             │            │
│  └─────────────┘  └─────────────┘  └─────────────┘            │
├─────────────────────────────────────────────────────────────────┤
│                      Data Layer                                 │
│  ┌─────────────┐  ┌─────────────┐  ┌─────────────┐            │
│  │ PostgreSQL  │  │    Redis    │  │ File Store  │            │
│  │             │  │             │  │             │            │
│  │ Primary DB  │  │   Cache     │  │   S3/Blob   │            │
│  │ Workflows   │  │  Sessions   │  │   Storage   │            │
│  │ Users       │  │  Rate Limit │  │   Assets    │            │
│  │ Executions  │  │             │  │             │            │
│  └─────────────┘  └─────────────┘  └─────────────┘            │
└─────────────────────────────────────────────────────────────────┘
```

## Directory Structure

```
backend/
├── alembic/                   # Database migrations
│   ├── versions/              # Migration files
│   │   └── 0001_initial_schema.py
│   ├── env.py                 # Alembic environment
│   └── script.py.mako         # Migration template
├── app/
│   ├── api/                   # API route handlers
│   │   ├── __init__.py
│   │   ├── auth.py            # Authentication endpoints
│   │   ├── workflows.py       # Workflow management
│   │   ├── templates.py       # Template management
│   │   ├── logs.py            # Execution logs
│   │   ├── monitoring.py      # System monitoring
│   │   └── websockets.py      # WebSocket handlers
│   ├── core/                  # Core application logic
│   │   ├── __init__.py
│   │   ├── config.py          # Configuration management
│   │   ├── security.py        # Security utilities
│   │   ├── dependencies.py    # FastAPI dependencies
│   │   └── exceptions.py      # Custom exceptions
│   ├── models/                # SQLAlchemy models
│   │   ├── __init__.py
│   │   ├── base.py            # Base model class
│   │   ├── user.py            # User model
│   │   ├── workflow.py        # Workflow models
│   │   ├── template.py        # Template models
│   │   └── execution.py       # Execution models
│   ├── schemas/               # Pydantic schemas
│   │   ├── __init__.py
│   │   ├── user.py            # User schemas
│   │   ├── workflow.py        # Workflow schemas
│   │   ├── template.py        # Template schemas
│   │   └── execution.py       # Execution schemas
│   ├── services/              # Business logic services
│   │   ├── __init__.py
│   │   ├── auth_service.py    # Authentication logic
│   │   ├── workflow_engine.py # Workflow execution
│   │   ├── template_engine.py # Template processing
│   │   ├── ai_service.py      # AI integration
│   │   └── email_service.py   # Email notifications
│   ├── utils/                 # Utility functions
│   │   ├── __init__.py
│   │   ├── database.py        # Database utilities
│   │   ├── logging.py         # Logging configuration
│   │   ├── validation.py      # Data validation
│   │   └── helpers.py         # Helper functions
│   ├── middleware/            # Custom middleware
│   │   ├── __init__.py
│   │   ├── cors.py            # CORS handling
│   │   ├── rate_limit.py      # Rate limiting
│   │   └── error_handler.py   # Error handling
│   ├── __init__.py
│   ├── main.py                # FastAPI application
│   ├── database.py            # Database configuration
│   └── auth.py                # Authentication utilities
├── tests/                     # Test suite
│   ├── __init__.py
│   ├── conftest.py            # Test configuration
│   ├── test_auth.py           # Authentication tests
│   ├── test_workflows.py      # Workflow tests
│   ├── test_templates.py      # Template tests
│   ├── test_ai_service.py     # AI service tests
│   └── integration/           # Integration tests
│       ├── __init__.py
│       ├── test_e2e_workflows.py
│       └── test_performance.py
├── requirements.txt           # Python dependencies
├── pyproject.toml            # Project configuration
├── Dockerfile                # Container configuration
├── docker-compose.yml        # Local development
└── alembic.ini               # Alembic configuration
```

## Core Components

### 1. FastAPI Application

#### Application Factory Pattern

```python
# app/main.py
from fastapi import FastAPI, Request
from fastapi.middleware.cors import CORSMiddleware
from fastapi.middleware.trustedhost import TrustedHostMiddleware
from contextlib import asynccontextmanager

from app.core.config import settings
from app.core.exceptions import setup_exception_handlers
from app.api import auth, workflows, templates, executions
from app.middleware.rate_limit import RateLimitMiddleware
from app.middleware.error_handler import ErrorHandlerMiddleware

@asynccontextmanager
async def lifespan(app: FastAPI):
    # Startup
    await startup_event()
    yield
    # Shutdown
    await shutdown_event()

def create_application() -> FastAPI:
    app = FastAPI(
        title="Auterity API",
        description="AI-powered workflow automation platform",
        version="1.0.0",
        docs_url="/docs" if settings.ENVIRONMENT != "production" else None,
        redoc_url="/redoc" if settings.ENVIRONMENT != "production" else None,
        lifespan=lifespan,
    )

    # Middleware
    app.add_middleware(
        CORSMiddleware,
        allow_origins=settings.CORS_ORIGINS,
        allow_credentials=True,
        allow_methods=["*"],
        allow_headers=["*"],
    )
    
    app.add_middleware(TrustedHostMiddleware, allowed_hosts=settings.ALLOWED_HOSTS)
    app.add_middleware(RateLimitMiddleware)
    app.add_middleware(ErrorHandlerMiddleware)

    # Exception handlers
    setup_exception_handlers(app)

    # Routers
    app.include_router(auth.router, prefix="/api/auth", tags=["authentication"])
    app.include_router(workflows.router, prefix="/api/workflows", tags=["workflows"])
    app.include_router(templates.router, prefix="/api/templates", tags=["templates"])
    app.include_router(executions.router, prefix="/api/executions", tags=["executions"])

    return app

app = create_application()

async def startup_event():
    """Initialize application on startup."""
    # Initialize database connections
    # Start background tasks
    # Load configuration
    pass

async def shutdown_event():
    """Cleanup on application shutdown."""
    # Close database connections
    # Stop background tasks
    pass
```

### 2. Database Layer

#### SQLAlchemy Models

```python
# app/models/base.py
from sqlalchemy import Column, DateTime, String, Boolean
from sqlalchemy.dialects.postgresql import UUID
from sqlalchemy.ext.declarative import declarative_base
from sqlalchemy.sql import func
import uuid

Base = declarative_base()

class BaseModel(Base):
    __abstract__ = True

    id = Column(UUID(as_uuid=True), primary_key=True, default=uuid.uuid4)
    created_at = Column(DateTime(timezone=True), server_default=func.now())
    updated_at = Column(DateTime(timezone=True), onupdate=func.now())
    is_active = Column(Boolean, default=True)

    def to_dict(self):
        return {c.name: getattr(self, c.name) for c in self.__table__.columns}
```

```python
# app/models/workflow.py
from sqlalchemy import Column, String, Text, ForeignKey, Enum, JSON
from sqlalchemy.orm import relationship
from sqlalchemy.dialects.postgresql import UUID
import enum

from .base import BaseModel

class WorkflowStatus(enum.Enum):
    DRAFT = "draft"
    ACTIVE = "active"
    INACTIVE = "inactive"
    ARCHIVED = "archived"

class Workflow(BaseModel):
    __tablename__ = "workflows"

    name = Column(String(255), nullable=False)
    description = Column(Text)
    user_id = Column(UUID(as_uuid=True), ForeignKey("users.id"), nullable=False)
    status = Column(Enum(WorkflowStatus), default=WorkflowStatus.DRAFT)
    definition = Column(JSON, nullable=False)
    
    # Relationships
    user = relationship("User", back_populates="workflows")
    executions = relationship("WorkflowExecution", back_populates="workflow")

class WorkflowExecution(BaseModel):
    __tablename__ = "workflow_executions"

    workflow_id = Column(UUID(as_uuid=True), ForeignKey("workflows.id"), nullable=False)
    status = Column(Enum(ExecutionStatus), default=ExecutionStatus.PENDING)
    input_data = Column(JSON)
    output_data = Column(JSON)
    error_message = Column(Text)
    started_at = Column(DateTime(timezone=True))
    completed_at = Column(DateTime(timezone=True))
    
    # Relationships
    workflow = relationship("Workflow", back_populates="executions")
    logs = relationship("ExecutionLog", back_populates="execution")
```

#### Database Configuration

```python
# app/database.py
from sqlalchemy import create_engine
from sqlalchemy.ext.asyncio import create_async_engine, AsyncSession
from sqlalchemy.orm import sessionmaker
from sqlalchemy.pool import NullPool

from app.core.config import settings

# Async engine for FastAPI
async_engine = create_async_engine(
    settings.DATABASE_URL.replace("postgresql://", "postgresql+asyncpg://"),
    echo=settings.DATABASE_ECHO,
    poolclass=NullPool if settings.ENVIRONMENT == "test" else None,
    pool_size=settings.DATABASE_POOL_SIZE,
    max_overflow=settings.DATABASE_MAX_OVERFLOW,
)

AsyncSessionLocal = sessionmaker(
    async_engine, 
    class_=AsyncSession, 
    expire_on_commit=False
)

# Sync engine for migrations
sync_engine = create_engine(
    settings.DATABASE_URL,
    echo=settings.DATABASE_ECHO,
    pool_size=settings.DATABASE_POOL_SIZE,
    max_overflow=settings.DATABASE_MAX_OVERFLOW,
)

async def get_db() -> AsyncSession:
    """Dependency to get database session."""
    async with AsyncSessionLocal() as session:
        try:
            yield session
        finally:
            await session.close()
```

### 3. Service Layer

#### Workflow Engine Service

```python
# app/services/workflow_engine.py
from typing import Dict, Any, List, Optional
from sqlalchemy.ext.asyncio import AsyncSession
from sqlalchemy import select
import asyncio
import logging

from app.models.workflow import Workflow, WorkflowExecution, ExecutionStatus
from app.models.execution import ExecutionLog
from app.services.ai_service import AIService
from app.core.exceptions import WorkflowExecutionError

logger = logging.getLogger(__name__)

class WorkflowEngine:
    def __init__(self, db: AsyncSession, ai_service: AIService):
        self.db = db
        self.ai_service = ai_service

    async def execute_workflow(
        self, 
        workflow_id: str, 
        input_data: Dict[str, Any]
    ) -> str:
        """Execute a workflow and return execution ID."""
        
        # Get workflow
        workflow = await self._get_workflow(workflow_id)
        if not workflow:
            raise WorkflowExecutionError(f"Workflow {workflow_id} not found")

        # Create execution record
        execution = WorkflowExecution(
            workflow_id=workflow_id,
            input_data=input_data,
            status=ExecutionStatus.PENDING
        )
        self.db.add(execution)
        await self.db.commit()
        await self.db.refresh(execution)

        # Execute workflow asynchronously
        asyncio.create_task(self._execute_workflow_steps(execution))

        return str(execution.id)

    async def _execute_workflow_steps(self, execution: WorkflowExecution):
        """Execute workflow steps sequentially."""
        try:
            execution.status = ExecutionStatus.RUNNING
            execution.started_at = func.now()
            await self.db.commit()

            workflow = await self._get_workflow(execution.workflow_id)
            steps = self._build_execution_order(workflow.definition)

            current_data = execution.input_data
            
            for step in steps:
                step_result = await self._execute_step(
                    execution.id, 
                    step, 
                    current_data
                )
                current_data = step_result.get('output', current_data)

            # Mark as completed
            execution.status = ExecutionStatus.COMPLETED
            execution.output_data = current_data
            execution.completed_at = func.now()
            
        except Exception as e:
            logger.error(f"Workflow execution failed: {e}")
            execution.status = ExecutionStatus.FAILED
            execution.error_message = str(e)
            execution.completed_at = func.now()
        
        finally:
            await self.db.commit()

    async def _execute_step(
        self, 
        execution_id: str, 
        step: Dict[str, Any], 
        input_data: Dict[str, Any]
    ) -> Dict[str, Any]:
        """Execute a single workflow step."""
        
        step_log = ExecutionLog(
            execution_id=execution_id,
            step_name=step.get('name', 'Unknown'),
            step_type=step.get('type', 'unknown'),
            input_data=input_data,
            timestamp=func.now()
        )

        try:
            start_time = time.time()
            
            # Route to appropriate step handler
            if step['type'] == 'ai_process':
                result = await self._execute_ai_step(step, input_data)
            elif step['type'] == 'data_transform':
                result = await self._execute_transform_step(step, input_data)
            elif step['type'] == 'condition':
                result = await self._execute_condition_step(step, input_data)
            else:
                result = await self._execute_default_step(step, input_data)

            # Log successful execution
            step_log.output_data = result
            step_log.duration_ms = int((time.time() - start_time) * 1000)
            
            return result

        except Exception as e:
            step_log.error_message = str(e)
            step_log.duration_ms = int((time.time() - start_time) * 1000)
            raise WorkflowExecutionError(f"Step {step['name']} failed: {e}")
        
        finally:
            self.db.add(step_log)
            await self.db.commit()

    async def _execute_ai_step(
        self, 
        step: Dict[str, Any], 
        input_data: Dict[str, Any]
    ) -> Dict[str, Any]:
        """Execute AI processing step."""
        
        prompt_template = step.get('prompt', '')
        model = step.get('model', 'gpt-3.5-turbo')
        
        # Substitute variables in prompt
        prompt = self._substitute_variables(prompt_template, input_data)
        
        # Call AI service
        response = await self.ai_service.process_text(prompt, model)
        
        return {
            'ai_response': response.content,
            'model_used': model,
            'tokens_used': response.usage.total_tokens if response.usage else 0,
            **input_data  # Pass through input data
        }

    async def get_execution_status(self, execution_id: str) -> Optional[Dict[str, Any]]:
        """Get execution status and details."""
        
        stmt = select(WorkflowExecution).where(WorkflowExecution.id == execution_id)
        result = await self.db.execute(stmt)
        execution = result.scalar_one_or_none()
        
        if not execution:
            return None

        return {
            'id': str(execution.id),
            'workflow_id': str(execution.workflow_id),
            'status': execution.status.value,
            'input_data': execution.input_data,
            'output_data': execution.output_data,
            'error_message': execution.error_message,
            'started_at': execution.started_at.isoformat() if execution.started_at else None,
            'completed_at': execution.completed_at.isoformat() if execution.completed_at else None,
        }

    async def cancel_execution(self, execution_id: str) -> bool:
        """Cancel a running workflow execution."""
        
        stmt = select(WorkflowExecution).where(WorkflowExecution.id == execution_id)
        result = await self.db.execute(stmt)
        execution = result.scalar_one_or_none()
        
        if not execution or execution.status != ExecutionStatus.RUNNING:
            return False

        execution.status = ExecutionStatus.CANCELLED
        execution.completed_at = func.now()
        await self.db.commit()
        
        return True
```

#### AI Service Integration

```python
# app/services/ai_service.py
from typing import Dict, Any, Optional, List
import openai
from openai import AsyncOpenAI
import logging

from app.core.config import settings
from app.core.exceptions import AIServiceError

logger = logging.getLogger(__name__)

class AIService:
    def __init__(self):
        self.client = AsyncOpenAI(api_key=settings.OPENAI_API_KEY)
        self.default_model = "gpt-3.5-turbo"
        self.max_tokens = 1000
        self.temperature = 0.7

    async def process_text(
        self, 
        prompt: str, 
        model: Optional[str] = None,
        context: Optional[Dict[str, Any]] = None
    ) -> Dict[str, Any]:
        """Process text using AI model."""
        
        try:
            model = model or self.default_model
            
            # Build messages
            messages = [
                {"role": "system", "content": self._get_system_prompt(context)},
                {"role": "user", "content": prompt}
            ]

            # Call OpenAI API
            response = await self.client.chat.completions.create(
                model=model,
                messages=messages,
                max_tokens=self.max_tokens,
                temperature=self.temperature,
                timeout=30.0
            )

            return {
                'content': response.choices[0].message.content,
                'model': model,
                'usage': {
                    'prompt_tokens': response.usage.prompt_tokens,
                    'completion_tokens': response.usage.completion_tokens,
                    'total_tokens': response.usage.total_tokens
                } if response.usage else None,
                'finish_reason': response.choices[0].finish_reason
            }

        except openai.APIError as e:
            logger.error(f"OpenAI API error: {e}")
            raise AIServiceError(f"AI processing failed: {e}")
        except Exception as e:
            logger.error(f"Unexpected AI service error: {e}")
            raise AIServiceError(f"AI service error: {e}")

    def _get_system_prompt(self, context: Optional[Dict[str, Any]] = None) -> str:
        """Get system prompt for AI model."""
        
        base_prompt = """You are an AI assistant for an automotive dealership workflow automation system. 
        You help process customer inquiries, service requests, and sales interactions.
        
        Always provide helpful, accurate, and professional responses.
        Format your responses in a clear, structured manner.
        """

        if context:
            dealership_name = context.get('dealership_name', 'the dealership')
            base_prompt += f"\n\nYou are representing {dealership_name}."
            
            if context.get('department'):
                base_prompt += f" You are assisting the {context['department']} department."

        return base_prompt

    async def generate_embeddings(self, texts: List[str]) -> List[List[float]]:
        """Generate embeddings for text similarity."""
        
        try:
            response = await self.client.embeddings.create(
                model="text-embedding-ada-002",
                input=texts
            )
            
            return [embedding.embedding for embedding in response.data]
            
        except Exception as e:
            logger.error(f"Embedding generation failed: {e}")
            raise AIServiceError(f"Embedding generation failed: {e}")
```

### 4. API Layer

#### Authentication Endpoints

```python
# app/api/auth.py
from fastapi import APIRouter, Depends, HTTPException, status
from fastapi.security import HTTPBearer, HTTPAuthorizationCredentials
from sqlalchemy.ext.asyncio import AsyncSession
from datetime import timedelta

from app.database import get_db
from app.schemas.user import UserCreate, UserLogin, UserResponse, Token
from app.services.auth_service import AuthService
from app.core.security import create_access_token, verify_token
from app.core.config import settings

router = APIRouter()
security = HTTPBearer()

@router.post("/register", response_model=UserResponse)
async def register(
    user_data: UserCreate,
    db: AsyncSession = Depends(get_db)
):
    """Register a new user."""
    auth_service = AuthService(db)
    
    # Check if user already exists
    existing_user = await auth_service.get_user_by_email(user_data.email)
    if existing_user:
        raise HTTPException(
            status_code=status.HTTP_400_BAD_REQUEST,
            detail="Email already registered"
        )
    
    # Create user
    user = await auth_service.create_user(user_data)
    return UserResponse.from_orm(user)

@router.post("/login", response_model=Token)
async def login(
    credentials: UserLogin,
    db: AsyncSession = Depends(get_db)
):
    """Authenticate user and return access token."""
    auth_service = AuthService(db)
    
    # Authenticate user
    user = await auth_service.authenticate_user(
        credentials.email, 
        credentials.password
    )
    
    if not user:
        raise HTTPException(
            status_code=status.HTTP_401_UNAUTHORIZED,
            detail="Invalid credentials",
            headers={"WWW-Authenticate": "Bearer"},
        )
    
    # Create access token
    access_token_expires = timedelta(minutes=settings.ACCESS_TOKEN_EXPIRE_MINUTES)
    access_token = create_access_token(
        data={"sub": str(user.id)}, 
        expires_delta=access_token_expires
    )
    
    return Token(
        access_token=access_token,
        token_type="bearer",
        expires_in=settings.ACCESS_TOKEN_EXPIRE_MINUTES * 60
    )

@router.get("/me", response_model=UserResponse)
async def get_current_user_info(
    credentials: HTTPAuthorizationCredentials = Depends(security),
    db: AsyncSession = Depends(get_db)
):
    """Get current user information."""
    # Verify token
    payload = verify_token(credentials.credentials)
    user_id = payload.get("sub")
    
    if not user_id:
        raise HTTPException(
            status_code=status.HTTP_401_UNAUTHORIZED,
            detail="Invalid token"
        )
    
    # Get user
    auth_service = AuthService(db)
    user = await auth_service.get_user_by_id(user_id)
    
    if not user:
        raise HTTPException(
            status_code=status.HTTP_404_NOT_FOUND,
            detail="User not found"
        )
    
    return UserResponse.from_orm(user)

@router.post("/refresh", response_model=Token)
async def refresh_token(
    credentials: HTTPAuthorizationCredentials = Depends(security),
    db: AsyncSession = Depends(get_db)
):
    """Refresh access token."""
    # Verify current token
    payload = verify_token(credentials.credentials)
    user_id = payload.get("sub")
    
    if not user_id:
        raise HTTPException(
            status_code=status.HTTP_401_UNAUTHORIZED,
            detail="Invalid token"
        )
    
    # Create new token
    access_token_expires = timedelta(minutes=settings.ACCESS_TOKEN_EXPIRE_MINUTES)
    access_token = create_access_token(
        data={"sub": user_id}, 
        expires_delta=access_token_expires
    )
    
    return Token(
        access_token=access_token,
        token_type="bearer",
        expires_in=settings.ACCESS_TOKEN_EXPIRE_MINUTES * 60
    )
```

### 5. Configuration Management

```python
# app/core/config.py
from pydantic_settings import BaseSettings
from typing import List, Optional
import os

class Settings(BaseSettings):
    # Application
    APP_NAME: str = "Auterity API"
    VERSION: str = "1.0.0"
    ENVIRONMENT: str = "development"
    DEBUG: bool = False
    
    # Security
    SECRET_KEY: str
    ACCESS_TOKEN_EXPIRE_MINUTES: int = 30
    REFRESH_TOKEN_EXPIRE_DAYS: int = 7
    
    # Database
    DATABASE_URL: str
    DATABASE_ECHO: bool = False
    DATABASE_POOL_SIZE: int = 10
    DATABASE_MAX_OVERFLOW: int = 20
    
    # Redis
    REDIS_URL: str = "redis://localhost:6379/0"
    
    # AI Services
    OPENAI_API_KEY: str
    OPENAI_MODEL: str = "gpt-3.5-turbo"
    
    # CORS
    CORS_ORIGINS: List[str] = ["http://localhost:3000"]
    ALLOWED_HOSTS: List[str] = ["*"]
    
    # Rate Limiting
    RATE_LIMIT_REQUESTS: int = 100
    RATE_LIMIT_WINDOW: int = 60  # seconds
    
    # Email
    SMTP_HOST: Optional[str] = None
    SMTP_PORT: int = 587
    SMTP_USERNAME: Optional[str] = None
    SMTP_PASSWORD: Optional[str] = None
    
    # File Storage
    FILE_STORAGE_TYPE: str = "local"  # local, s3, azure
    FILE_STORAGE_BUCKET: Optional[str] = None
    
    # Monitoring
    SENTRY_DSN: Optional[str] = None
    LOG_LEVEL: str = "INFO"
    
    class Config:
        env_file = ".env"
        case_sensitive = True

settings = Settings()
```

### 6. Testing Framework

#### Test Configuration

```python
# tests/conftest.py
import pytest
import asyncio
from httpx import AsyncClient
from sqlalchemy.ext.asyncio import create_async_engine, AsyncSession
from sqlalchemy.orm import sessionmaker
from sqlalchemy.pool import StaticPool

from app.main import app
from app.database import get_db
from app.models.base import Base
from app.core.config import settings

# Test database URL
TEST_DATABASE_URL = "sqlite+aiosqlite:///./test.db"

@pytest.fixture(scope="session")
def event_loop():
    """Create an instance of the default event loop for the test session."""
    loop = asyncio.get_event_loop_policy().new_event_loop()
    yield loop
    loop.close()

@pytest.fixture(scope="session")
async def test_engine():
    """Create test database engine."""
    engine = create_async_engine(
        TEST_DATABASE_URL,
        connect_args={"check_same_thread": False},
        poolclass=StaticPool,
    )
    
    async with engine.begin() as conn:
        await conn.run_sync(Base.metadata.create_all)
    
    yield engine
    
    async with engine.begin() as conn:
        await conn.run_sync(Base.metadata.drop_all)

@pytest.fixture
async def db_session(test_engine):
    """Create test database session."""
    TestSessionLocal = sessionmaker(
        test_engine, class_=AsyncSession, expire_on_commit=False
    )
    
    async with TestSessionLocal() as session:
        yield session

@pytest.fixture
async def client(db_session):
    """Create test client."""
    app.dependency_overrides[get_db] = lambda: db_session
    
    async with AsyncClient(app=app, base_url="http://test") as ac:
        yield ac
    
    app.dependency_overrides.clear()

@pytest.fixture
async def authenticated_client(client, db_session):
    """Create authenticated test client."""
    # Create test user
    user_data = {
        "email": "test@example.com",
        "password": "testpassword",
        "name": "Test User"
    }
    
    # Register user
    await client.post("/api/auth/register", json=user_data)
    
    # Login and get token
    login_response = await client.post("/api/auth/login", json={
        "email": user_data["email"],
        "password": user_data["password"]
    })
    
    token = login_response.json()["access_token"]
    client.headers.update({"Authorization": f"Bearer {token}"})
    
    return client
```

#### Integration Tests

```python
# tests/test_workflow_integration.py
import pytest
from httpx import AsyncClient

@pytest.mark.asyncio
async def test_workflow_creation_and_execution(authenticated_client: AsyncClient):
    """Test complete workflow creation and execution flow."""
    
    # Create workflow
    workflow_data = {
        "name": "Test Workflow",
        "description": "Integration test workflow",
        "definition": {
            "nodes": [
                {
                    "id": "start",
                    "type": "start",
                    "data": {"label": "Start"}
                },
                {
                    "id": "ai_process",
                    "type": "ai_process",
                    "data": {
                        "label": "AI Process",
                        "prompt": "Process this text: {{input_text}}"
                    }
                },
                {
                    "id": "end",
                    "type": "end",
                    "data": {"label": "End"}
                }
            ],
            "edges": [
                {"source": "start", "target": "ai_process"},
                {"source": "ai_process", "target": "end"}
            ]
        }
    }
    
    # Create workflow
    create_response = await authenticated_client.post(
        "/api/workflows", 
        json=workflow_data
    )
    assert create_response.status_code == 201
    workflow = create_response.json()
    workflow_id = workflow["id"]
    
    # Execute workflow
    execution_data = {
        "input_data": {
            "input_text": "Hello, this is a test message"
        }
    }
    
    execute_response = await authenticated_client.post(
        f"/api/workflows/{workflow_id}/execute",
        json=execution_data
    )
    assert execute_response.status_code == 200
    execution = execute_response.json()
    execution_id = execution["execution_id"]
    
    # Check execution status
    status_response = await authenticated_client.get(
        f"/api/executions/{execution_id}"
    )
    assert status_response.status_code == 200
    status = status_response.json()
    assert status["status"] in ["pending", "running", "completed"]
    
    # Get execution logs
    logs_response = await authenticated_client.get(
        f"/api/executions/{execution_id}/logs"
    )
    assert logs_response.status_code == 200
    logs = logs_response.json()
    assert isinstance(logs, list)
```

## Performance Optimization

### Database Optimization

```python
# Database connection pooling and optimization
from sqlalchemy import event
from sqlalchemy.engine import Engine
import time

@event.listens_for(Engine, "before_cursor_execute")
def receive_before_cursor_execute(conn, cursor, statement, parameters, context, executemany):
    context._query_start_time = time.time()

@event.listens_for(Engine, "after_cursor_execute")
def receive_after_cursor_execute(conn, cursor, statement, parameters, context, executemany):
    total = time.time() - context._query_start_time
    if total > 0.1:  # Log slow queries
        logger.warning(f"Slow query: {total:.2f}s - {statement[:100]}")

# Index optimization
class Workflow(BaseModel):
    __tablename__ = "workflows"
    
    # Add indexes for common queries
    __table_args__ = (
        Index('idx_workflows_user_id', 'user_id'),
        Index('idx_workflows_status', 'status'),
        Index('idx_workflows_created_at', 'created_at'),
        Index('idx_workflows_name_search', 'name', postgresql_using='gin'),
    )
```

### Caching Strategy

```python
# app/utils/cache.py
import redis
import json
from typing import Any, Optional
from functools import wraps

redis_client = redis.from_url(settings.REDIS_URL)

def cache_result(expiration: int = 300):
    """Decorator to cache function results."""
    def decorator(func):
        @wraps(func)
        async def wrapper(*args, **kwargs):
            # Generate cache key
            cache_key = f"{func.__name__}:{hash(str(args) + str(kwargs))}"
            
            # Try to get from cache
            cached_result = redis_client.get(cache_key)
            if cached_result:
                return json.loads(cached_result)
            
            # Execute function and cache result
            result = await func(*args, **kwargs)
            redis_client.setex(
                cache_key, 
                expiration, 
                json.dumps(result, default=str)
            )
            
            return result
        return wrapper
    return decorator

# Usage example
@cache_result(expiration=600)  # Cache for 10 minutes
async def get_workflow_templates(category: Optional[str] = None):
    # Expensive database query
    pass
```

### Async Processing

```python
# app/services/background_tasks.py
from celery import Celery
from app.core.config import settings

celery_app = Celery(
    "auterity",
    broker=settings.REDIS_URL,
    backend=settings.REDIS_URL,
    include=["app.tasks"]
)

# Configuration
celery_app.conf.update(
    task_serializer="json",
    accept_content=["json"],
    result_serializer="json",
    timezone="UTC",
    enable_utc=True,
    task_track_started=True,
    task_time_limit=30 * 60,  # 30 minutes
    task_soft_time_limit=25 * 60,  # 25 minutes
    worker_prefetch_multiplier=1,
    worker_max_tasks_per_child=1000,
)

# Background task example
@celery_app.task(bind=True)
def execute_workflow_async(self, workflow_id: str, input_data: dict):
    """Execute workflow in background."""
    try:
        # Workflow execution logic
        pass
    except Exception as exc:
        # Retry with exponential backoff
        raise self.retry(exc=exc, countdown=60, max_retries=3)
```

---

**Document Version**: 1.0  
**Last Updated**: $(date)  
**Architecture Review**: Monthly backend architecture assessment  
**Maintained By**: Auterity Backend Team